{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from flair.data import  Corpus\n",
    "from flair.datasets import ColumnCorpus, DataLoader, ColumnDataset\n",
    "from flair.models import SequenceTagger\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-03 10:27:12,996 loading file ./resources/taggers/Test2/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = './resources/taggers/Test2/best-model.pt'\n",
    "model : SequenceTagger = SequenceTagger.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"There’s no doubt that this is an extremely strange and frankly fishy situation; something doesn’t add up, it’s true.\"   [− Tokens: 19  − Token-Labels: \"There’s <0> no <0> doubt <0> that <0> this <0> is <0> an <0> extremely <0> strange <0> and <0> frankly <1> fishy <1> situation; <0> something <0> doesn’t <0> add <0> up, <0> it’s <0> true. <0>\"]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"There’s no doubt that this is an extremely strange and frankly fishy situation; something doesn’t add up, it’s true.\"\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"That's what Columbia snowflakes thought was offensive.\"   [− Tokens: 7  − Token-Labels: \"That's <0> what <0> Columbia <0> snowflakes <0> thought <0> was <0> offensive. <0>\"]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"That's what Columbia snowflakes thought was offensive.\"\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"Comrades, these jokes you have been listening to are thought crimes!\"   [− Tokens: 11  − Token-Labels: \"Comrades, <0> these <0> jokes <0> you <0> have <0> been <0> listening <0> to <0> are <0> thought <0> crimes! <1>\"]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Comrades, these jokes you have been listening to are thought crimes!\"\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"I'm sure Patel felt very, like, accepted.\"   [− Tokens: 7  − Token-Labels: \"I'm <0> sure <0> Patel <0> felt <0> very, <0> like, <0> accepted. <0>\"]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I'm sure Patel felt very, like, accepted.\"\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"However, what Comey really did was to give Hillary Clinton a Get Out of Jail Free card.\"   [− Tokens: 17  − Token-Labels: \"However, <0> what <0> Comey <0> really <0> did <0> was <0> to <0> give <0> Hillary <0> Clinton <0> a <0> Get <0> Out <0> of <0> Jail <0> Free <0> card. <0>\"]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"However, what Comey really did was to give Hillary Clinton a Get Out of Jail Free card.\"\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"It turns out that some of the e-mails were accessed by foreign parties.\"   [− Tokens: 13  − Token-Labels: \"It <0> turns <0> out <0> that <0> some <0> of <0> the <0> e-mails <0> were <0> accessed <0> by <0> foreign <0> parties. <0>\"]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sve nule\n",
    "sentence = \"It turns out that some of the e-mails were accessed by foreign parties.\"\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"Still, the delay prompted critics to speculate that the delay signaled the White House was having second thoughts about the nomination.\"   [− Tokens: 21  − Token-Labels: \"Still, <0> the <0> delay <0> prompted <0> critics <0> to <0> speculate <0> that <0> the <0> delay <0> signaled <0> the <0> White <0> House <0> was <0> having <0> second <0> thoughts <0> about <0> the <0> nomination. <0>\"]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Still, the delay prompted critics to speculate that the delay signaled the White House was having second thoughts about the nomination.\"\n",
    "model.predict(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
