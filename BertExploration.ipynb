{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_index(sentence, word):\n",
    "    index = sentence.split(\" \").index(word)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def measure_similarity(a, b):\n",
    "    cosine_value = 1 - cosine(a, b)\n",
    "    euclid_value = np.linalg.norm(a-b)\n",
    "    return cosine_value, euclid_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBatchEmbedding:\n",
    "    def __init__(self):\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True).eval().cuda()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        padded_sequence = self.tokenizer.batch_encode_plus(sentences, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "        out = self.model(padded_sequence['input_ids'].cuda(), padded_sequence[\"attention_mask\"].cuda())\n",
    "        hidden_states = out[2]\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings = torch.flatten(token_embeddings.permute(1, 2, 0,3), start_dim=2)[:,1:-1,:]\n",
    "        return token_embeddings.cpu().detach().numpy()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "    \n",
    "bert = BertBatchEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "    def tokenize(self, sentences : list):\n",
    "        encoded = []\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.tokenize(sentence)\n",
    "            encoded_sentence = []\n",
    "            for token in tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    encoded_sentence.append(self.vocabulary[token])\n",
    "                else:\n",
    "                    encoded_sentence.append(self.vocabulary[\"<UNK>\"])\n",
    "            encoded.append(encoded_sentence)\n",
    "        return torch.tensor(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertEmbedding:\n",
    "    def __init__(self):\n",
    "        with open('datasets/weights.pickle', 'rb') as handle:\n",
    "            weights = torch.tensor(pickle.load(handle))\n",
    "            \n",
    "        with open('datasets/vocab.pickle', 'rb') as handle:\n",
    "            vocabulary = pickle.load(handle)\n",
    "            \n",
    "        self.model = nn.Embedding.from_pretrained(weights, padding_idx = 0, freeze = True)\n",
    "        self.tokenizer = CustomTokenizer(vocabulary)\n",
    "        \n",
    "    def transform(self, text):\n",
    "        tokenized_text = self.tokenizer.tokenize(text).long()\n",
    "        embedded_text = self.model(tokenized_text)\n",
    "        return embedded_text\n",
    "    \n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "    \n",
    "custom = CustomBertEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context hypotehesis testing\n",
    "- BERT encodes words based on context meaning that the model looks at the whole sentence and encodes the word based on the meaining it has in the sentence. Words that are in a more similar context are more similar.\n",
    "- We also need to define what the core meaining of the word is. The core meaning of the word is the meaining that the word has regardless of the context. The only thing the word knows is its true meaning regarding to polysemi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_sentence = \"He brutally killed someone\"\n",
    "embedding = bert.transform([max_sentence])[0]\n",
    "max_embedding = embedding[embedding_index(max_sentence,\"killed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sentence = \"He accidentally killed someone\"\n",
    "embedding = bert.transform([min_sentence])[0]\n",
    "min_embedding = embedding[embedding_index(min_sentence,\"killed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indif_sentence = \"He killed someone\"\n",
    "embedding = bert.transform([indif_sentence])[0]\n",
    "indif_embedding = embedding[embedding_index(indif_sentence,\"killed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_sentence = \"Someone was killed in a horrifying manner\"\n",
    "embedding = bert.transform([huge_sentence])[0]\n",
    "huge_embedding = embedding[embedding_index(huge_sentence, \"killed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7988684177398682, 43.771984)\n",
      "(0.7915605306625366, 44.648533)\n",
      "(0.7995526790618896, 43.83039)\n"
     ]
    }
   ],
   "source": [
    "print(measure_similarity(max_embedding, huge_embedding))\n",
    "print(measure_similarity(min_embedding, huge_embedding))\n",
    "print(measure_similarity(indif_embedding, huge_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_sentence = \"killed\"\n",
    "embedding = custom.transform([core_sentence])[0].data.numpy()\n",
    "core_embedding = embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8312858157107608, 38.07012018152099)\n",
      "(0.8240340527046277, 38.98820765753455)\n",
      "(0.8210544978745334, 39.37731779561605)\n"
     ]
    }
   ],
   "source": [
    "print(measure_similarity(max_embedding, core_embedding))\n",
    "print(measure_similarity(min_embedding, core_embedding))\n",
    "print(measure_similarity(indif_embedding, core_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(propaganda, nonpropaganda, word):\n",
    "    embedding = bert.transform([propaganda])[0]\n",
    "    propaganda = embedding[embedding_index(propaganda, word)][-768:]\n",
    "    embedding = bert.transform([nonpropaganda])[0]\n",
    "    nonpropaganda = embedding[embedding_index(nonpropaganda, word)][-768:]\n",
    "    embedding = bert.transform([word])[0]\n",
    "    bert_core_embedding = embedding[0][-768:]\n",
    "    embedding = custom.transform([word])[0].data.numpy()\n",
    "    core_embedding = embedding[0][-768:]\n",
    "    print(\"Differences between propaganda and non propaganda\")\n",
    "    print(\"Using core embeddings\",\"cosine\",abs(measure_similarity(propaganda, core_embedding)[0]-measure_similarity(nonpropaganda, core_embedding)[0]),\"euclid\",abs(measure_similarity(propaganda, core_embedding)[1]-measure_similarity(nonpropaganda, core_embedding)[1]))\n",
    "    print(\"Using bert core embeddings\",\"cosine\", abs(measure_similarity(propaganda, bert_core_embedding)[0]-measure_similarity(nonpropaganda, bert_core_embedding)[0]),\"euclid\",abs(measure_similarity(propaganda, bert_core_embedding)[1]-measure_similarity(nonpropaganda, bert_core_embedding)[1]))\n",
    "    print(\"Direct difference between words\",\"cosine\",1-measure_similarity(nonpropaganda, propaganda)[0], \"euclid\",measure_similarity(nonpropaganda, propaganda)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "propaganda = \"In the future everyone will be the subject of social justice crybullying for 15 minutes.\"\n",
    "nonpropaganda = \"In the future everyone will be the subject of social justice for 15 minutes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between propaganda and non propaganda\n",
      "Using core embeddings cosine 0.02750954261729066 euclid 0.14267734322225145\n",
      "Using bert core embeddings cosine 0.04128524661064148 euclid 0.7919636\n",
      "Direct difference between words cosine 0.08723562955856323 euclid 5.8261724\n"
     ]
    }
   ],
   "source": [
    "word_similarity(propaganda,nonpropaganda, \"social\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between propaganda and non propaganda\n",
      "Using core embeddings cosine 0.00807510322788807 euclid 0.07085715982751317\n",
      "Using bert core embeddings cosine 0.04355967044830322 euclid 0.6902571\n",
      "Direct difference between words cosine 0.1435481309890747 euclid 6.940008\n"
     ]
    }
   ],
   "source": [
    "word_similarity(propaganda,nonpropaganda, \"justice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "propaganda = \"During the event, Patel’s performance featured commentary on his experience living in a diverse area of New York\"\n",
    "nonpropaganda = \"During the event, Patel’s performance featured commentary on his experience living in a area of New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between propaganda and non propaganda\n",
      "Using core embeddings cosine 0.03688372388358874 euclid 0.9201254427506029\n",
      "Using bert core embeddings cosine 0.007760718464851379 euclid 0.5483227\n",
      "Direct difference between words cosine 0.22605055570602417 euclid 9.4450445\n"
     ]
    }
   ],
   "source": [
    "word_similarity(propaganda,nonpropaganda, \"area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "propaganda = \"That's what Columbia snowflakes thought was offensive\"\n",
    "nonpropaganda = \"That's what Columbia thought was offensive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between propaganda and non propaganda\n",
      "Using core embeddings cosine 0.005112637015793564 euclid 0.19526626633392397\n",
      "Using bert core embeddings cosine 0.01566094160079956 euclid 0.05400467\n",
      "Direct difference between words cosine 0.05840563774108887 euclid 5.2522326\n"
     ]
    }
   ],
   "source": [
    "word_similarity(propaganda,nonpropaganda, \"Columbia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between propaganda and non propaganda\n",
      "Using core embeddings cosine 0.14327158173455334 euclid 3.951414532332768\n",
      "Using bert core embeddings cosine 0.15078258514404297 euclid 3.8719482\n",
      "Direct difference between words cosine 0.7403800189495087 euclid 18.854862\n"
     ]
    }
   ],
   "source": [
    "word_similarity(propaganda,nonpropaganda, \"offensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "propaganda = \"It’s got to be either one of the stupidest acts that I can recall or a very wicked plan by Washington neocons to sabotage Korean peace talks.\"\n",
    "nonpropaganda = \"It’s got to be either one of the acts that I can recall or a very wicked plan by Washington neocons to sabotage Korean peace talks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between propaganda and non propaganda\n",
      "Using core embeddings cosine 0.09871787939178611 euclid 2.281394350262664\n",
      "Using bert core embeddings cosine 0.0027364641427993774 euclid 1.0827847\n",
      "Direct difference between words cosine 0.40493500232696533 euclid 14.424915\n"
     ]
    }
   ],
   "source": [
    "word_similarity(propaganda,nonpropaganda, \"acts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\"> What would be the best way to represent the core meaning of a word so that we can capture the differences between the core meaning and the meaning inside the context?</font>\n",
    "> - <font size=\"3\"> We can use an embedding which we create from averaging multiple vector embbedings from the same word inside multiple contexts</font>\n",
    "> - <font size=\"3\"> We can use the basic embbeding of the word without any context at all. This would be the most robust solution since bert probably outputs the most generic embeddings.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert batch embedding\n",
    "Since we will feed mutliple sentences in batches we need a way to preprocess the sentences in batches instead of sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.92\n",
      "Vector similarity for *different* meanings:  0.71\n"
     ]
    }
   ],
   "source": [
    "text = [\"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "embeddings = bert.transform(text)[0]\n",
    "diff_bank = 1 - cosine(embeddings[9], embeddings[18])\n",
    "same_bank = 1 - cosine(embeddings[9], embeddings[5])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to create vector embeddings from bert. You can avarage all the hidden layers. You can also avarage the last 4 layers. You can concat the last 4 layers instead of averaging. So this is also a part of our work that will need a lot of testing. We should probably create a table that contains all the different vector embeddings and compare their preformance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"train\"\n",
    "data = open(\"datasets/Processed/\"+filename+\".txt\", \"r\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "x = []\n",
    "y = []\n",
    "for i in data:\n",
    "    if i==\"\":\n",
    "        X.append(\" \".join(x))\n",
    "        Y.append(y)\n",
    "        x = []\n",
    "        y = []\n",
    "    else:\n",
    "        row = i.split(\" \")\n",
    "        x.append(row[0])\n",
    "        y.append(int(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "Y_new = []\n",
    "X_new = []\n",
    "locations = []\n",
    "special = []\n",
    "for x,y in zip(X,Y):\n",
    "    tokenized = tokenizer.tokenize(x)\n",
    "    X_new.append(tokenized)\n",
    "    split_x = x.lower().split(\" \")\n",
    "    counter = 0\n",
    "    new_y = []\n",
    "    local = []\n",
    "    local_special = []\n",
    "    word = \"\"\n",
    "    for i, token in enumerate(tokenized):\n",
    "        word = word + token.replace(\"#\",\"\")\n",
    "        if word == split_x[counter]:\n",
    "            new_y.append(y[counter])\n",
    "            counter+=1\n",
    "            word = \"\"\n",
    "            if \"#\" in token:\n",
    "                local.append(i)\n",
    "        else:\n",
    "            new_y.append(y[counter])\n",
    "            if \"#\" in token:\n",
    "                local.append(i)\n",
    "        if re.findall('[^A-Za-z0-9]',token) and not \"#\" in token:\n",
    "            local_special.append(i)\n",
    "    locations.append(local)\n",
    "    special.append(local_special)\n",
    "    assert len(new_y)!=tokenized\n",
    "    for i,j in enumerate(tokenized):\n",
    "        if not re.search('[a-zA-Z]', j):\n",
    "            new_y[i] = 0\n",
    "    Y_new.append(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std(df, lenght):\n",
    "    cos_1 = df.loc[(df[\"label\"]==1) & (df['token'].str.len()>lenght) & (df[\"is_word\"]==1)][\"cosine_value\"].values\n",
    "    cos_0 = df.loc[(df[\"label\"]==0) & (df['token'].str.len()>lenght) & (df[\"is_word\"]==1)][\"cosine_value\"].values\n",
    "    euclid_1 = df.loc[(df[\"label\"]==1) & (df['token'].str.len()>lenght) & (df[\"is_word\"]==1)][\"euclid_value\"].values\n",
    "    euclid_0 = df.loc[(df[\"label\"]==0) & (df['token'].str.len()>lenght) & (df[\"is_word\"]==1)][\"euclid_value\"].values\n",
    "    sns.distplot(cos_1, hist=False, label=\"1\")\n",
    "    sns.distplot(cos_0, hist=False, label=\"0\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    print(\"1:\",cos_1.mean(),cos_1.std(), euclid_1.mean(), euclid_1.std())\n",
    "    sns.distplot(euclid_1, hist=False, label=\"1\")\n",
    "    sns.distplot(euclid_0, hist=False, label=\"0\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    print(\"0:\",cos_0.mean(),cos_0.std(), euclid_0.mean(), euclid_0.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'the': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'to': 5,\n",
       " 'of': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'a': 9,\n",
       " 'that': 10,\n",
       " '’': 11,\n",
       " 's': 12,\n",
       " 'is': 13,\n",
       " '“': 14,\n",
       " '”': 15,\n",
       " '-': 16,\n",
       " 'for': 17,\n",
       " 'it': 18,\n",
       " 'on': 19,\n",
       " 'was': 20,\n",
       " 'he': 21,\n",
       " 'with': 22,\n",
       " 'as': 23,\n",
       " 'this': 24,\n",
       " 'be': 25,\n",
       " 'by': 26,\n",
       " 'not': 27,\n",
       " 'his': 28,\n",
       " 'have': 29,\n",
       " 'are': 30,\n",
       " '##s': 31,\n",
       " ':': 32,\n",
       " 'has': 33,\n",
       " 'i': 34,\n",
       " 'from': 35,\n",
       " 'at': 36,\n",
       " 'they': 37,\n",
       " 'who': 38,\n",
       " \"'\": 39,\n",
       " 'an': 40,\n",
       " 'said': 41,\n",
       " '\"': 42,\n",
       " 'but': 43,\n",
       " 'we': 44,\n",
       " 'you': 45,\n",
       " '?': 46,\n",
       " 't': 47,\n",
       " 'will': 48,\n",
       " 'trump': 49,\n",
       " 'or': 50,\n",
       " 'had': 51,\n",
       " ')': 52,\n",
       " '(': 53,\n",
       " 'all': 54,\n",
       " 'were': 55,\n",
       " 'about': 56,\n",
       " 'their': 57,\n",
       " 'what': 58,\n",
       " 'one': 59,\n",
       " 'which': 60,\n",
       " 'been': 61,\n",
       " 'no': 62,\n",
       " 'our': 63,\n",
       " 'out': 64,\n",
       " 'there': 65,\n",
       " 'if': 66,\n",
       " '—': 67,\n",
       " 'would': 68,\n",
       " 'so': 69,\n",
       " 'people': 70,\n",
       " 'do': 71,\n",
       " 'also': 72,\n",
       " 'she': 73,\n",
       " 'when': 74,\n",
       " 'after': 75,\n",
       " 'up': 76,\n",
       " 'her': 77,\n",
       " 'more': 78,\n",
       " 'should': 79,\n",
       " 'us': 80,\n",
       " 'can': 81,\n",
       " 'president': 82,\n",
       " 'him': 83,\n",
       " 'church': 84,\n",
       " 'now': 85,\n",
       " 'them': 86,\n",
       " 'over': 87,\n",
       " 'its': 88,\n",
       " 'u': 89,\n",
       " 'new': 90,\n",
       " 'pope': 91,\n",
       " '–': 92,\n",
       " 'against': 93,\n",
       " 'any': 94,\n",
       " 'other': 95,\n",
       " 'time': 96,\n",
       " 'only': 97,\n",
       " 'into': 98,\n",
       " 'even': 99,\n",
       " 'because': 100,\n",
       " 'may': 101,\n",
       " 'just': 102,\n",
       " 'ka': 103,\n",
       " 'than': 104,\n",
       " '##ing': 105,\n",
       " '##ly': 106,\n",
       " '/': 107,\n",
       " 'house': 108,\n",
       " 'my': 109,\n",
       " 'those': 110,\n",
       " 'like': 111,\n",
       " 'years': 112,\n",
       " 'these': 113,\n",
       " 'state': 114,\n",
       " 'did': 115,\n",
       " 'some': 116,\n",
       " 'being': 117,\n",
       " '##ed': 118,\n",
       " 'how': 119,\n",
       " 'told': 120,\n",
       " 'two': 121,\n",
       " 'before': 122,\n",
       " 'first': 123,\n",
       " 'report': 124,\n",
       " '##ugh': 125,\n",
       " 'could': 126,\n",
       " '##vana': 127,\n",
       " 'me': 128,\n",
       " 'many': 129,\n",
       " 'then': 130,\n",
       " 'iran': 131,\n",
       " 'most': 132,\n",
       " '[': 133,\n",
       " ']': 134,\n",
       " 'police': 135,\n",
       " 'public': 136,\n",
       " 'know': 137,\n",
       " 'very': 138,\n",
       " 'white': 139,\n",
       " '!': 140,\n",
       " 'news': 141,\n",
       " 'government': 142,\n",
       " 'american': 143,\n",
       " 'while': 144,\n",
       " 'law': 145,\n",
       " 'media': 146,\n",
       " 'made': 147,\n",
       " 'year': 148,\n",
       " '##er': 149,\n",
       " 'why': 150,\n",
       " 'far': 151,\n",
       " 'man': 152,\n",
       " 'francis': 153,\n",
       " 'well': 154,\n",
       " 'court': 155,\n",
       " 'right': 156,\n",
       " 'clinton': 157,\n",
       " '##t': 158,\n",
       " 'back': 159,\n",
       " 'un': 160,\n",
       " 'fbi': 161,\n",
       " '…': 162,\n",
       " '*': 163,\n",
       " 'world': 164,\n",
       " 'obama': 165,\n",
       " 're': 166,\n",
       " 'left': 167,\n",
       " 'states': 168,\n",
       " 'case': 169,\n",
       " 'take': 170,\n",
       " 'cardinal': 171,\n",
       " ';': 172,\n",
       " 'say': 173,\n",
       " 'see': 174,\n",
       " '‘': 175,\n",
       " 'under': 176,\n",
       " 'during': 177,\n",
       " 'investigation': 178,\n",
       " 'last': 179,\n",
       " 'don': 180,\n",
       " 'according': 181,\n",
       " 'free': 182,\n",
       " 'story': 183,\n",
       " 'where': 184,\n",
       " 'god': 185,\n",
       " 'go': 186,\n",
       " '##a': 187,\n",
       " 'such': 188,\n",
       " 'press': 189,\n",
       " 'get': 190,\n",
       " 'gun': 191,\n",
       " 'catholic': 192,\n",
       " 'way': 193,\n",
       " '##r': 194,\n",
       " 'former': 195,\n",
       " 'united': 196,\n",
       " 'evidence': 197,\n",
       " 'fact': 198,\n",
       " 'must': 199,\n",
       " 'life': 200,\n",
       " 'department': 201,\n",
       " 'never': 202,\n",
       " 'email': 203,\n",
       " '##d': 204,\n",
       " 'going': 205,\n",
       " 'here': 206,\n",
       " 'come': 207,\n",
       " '##y': 208,\n",
       " 'abuse': 209,\n",
       " '1': 210,\n",
       " 'judge': 211,\n",
       " 'another': 212,\n",
       " 'same': 213,\n",
       " 'still': 214,\n",
       " 'country': 215,\n",
       " 'make': 216,\n",
       " 'both': 217,\n",
       " 'day': 218,\n",
       " 'does': 219,\n",
       " 'poll': 220,\n",
       " 'administration': 221,\n",
       " 'since': 222,\n",
       " 'order': 223,\n",
       " 'bishops': 224,\n",
       " 'cia': 225,\n",
       " 'own': 226,\n",
       " 'part': 227,\n",
       " 'anti': 228,\n",
       " 'con': 229,\n",
       " '##o': 230,\n",
       " 'think': 231,\n",
       " 'security': 232,\n",
       " 'good': 233,\n",
       " 'federal': 234,\n",
       " 'including': 235,\n",
       " 'russia': 236,\n",
       " '2018': 237,\n",
       " 'your': 238,\n",
       " 'policy': 239,\n",
       " 'information': 240,\n",
       " 'without': 241,\n",
       " 'military': 242,\n",
       " 'says': 243,\n",
       " '##han': 244,\n",
       " 'justice': 245,\n",
       " 'want': 246,\n",
       " '##an': 247,\n",
       " 'shooting': 248,\n",
       " 'called': 249,\n",
       " 'election': 250,\n",
       " 'attorney': 251,\n",
       " 'much': 252,\n",
       " 'officials': 253,\n",
       " 'america': 254,\n",
       " 'mcc': 255,\n",
       " 'reported': 256,\n",
       " '_': 257,\n",
       " 'down': 258,\n",
       " 'border': 259,\n",
       " 'article': 260,\n",
       " 'ass': 261,\n",
       " '##ar': 262,\n",
       " 'through': 263,\n",
       " 'general': 264,\n",
       " 'long': 265,\n",
       " 'letter': 266,\n",
       " 'washington': 267,\n",
       " 'war': 268,\n",
       " 'e': 269,\n",
       " 'islam': 270,\n",
       " 'jihad': 271,\n",
       " 'national': 272,\n",
       " 'access': 273,\n",
       " 'john': 274,\n",
       " 'men': 275,\n",
       " 'office': 276,\n",
       " 'campaign': 277,\n",
       " 'between': 278,\n",
       " 'deal': 279,\n",
       " 'today': 280,\n",
       " 'three': 281,\n",
       " '##g': 282,\n",
       " 'use': 283,\n",
       " 'statement': 284,\n",
       " 'nuclear': 285,\n",
       " 'muslim': 286,\n",
       " 'saying': 287,\n",
       " '@': 288,\n",
       " 'ford': 289,\n",
       " 'used': 290,\n",
       " '##rick': 291,\n",
       " 'support': 292,\n",
       " 'history': 293,\n",
       " 'm': 294,\n",
       " 'freedom': 295,\n",
       " 'ac': 296,\n",
       " 'sexual': 297,\n",
       " '2': 298,\n",
       " '##osta': 299,\n",
       " 'democrat': 300,\n",
       " 'every': 301,\n",
       " 'believe': 302,\n",
       " 'end': 303,\n",
       " 'democratic': 304,\n",
       " 'sc': 305,\n",
       " 'himself': 306,\n",
       " 'place': 307,\n",
       " '##is': 308,\n",
       " 'continues': 309,\n",
       " 'death': 310,\n",
       " 'party': 311,\n",
       " 'charge': 312,\n",
       " 'person': 313,\n",
       " '##i': 314,\n",
       " 'found': 315,\n",
       " 'confirmed': 316,\n",
       " '##ange': 317,\n",
       " '000': 318,\n",
       " 'home': 319,\n",
       " 'old': 320,\n",
       " 'illegal': 321,\n",
       " 'stop': 322,\n",
       " 'criminal': 323,\n",
       " 'political': 324,\n",
       " 'd': 325,\n",
       " 'work': 326,\n",
       " 'clear': 327,\n",
       " 'ex': 328,\n",
       " 'com': 329,\n",
       " 'vatican': 330,\n",
       " 'wrote': 331,\n",
       " 'ellison': 332,\n",
       " 'however': 333,\n",
       " 'power': 334,\n",
       " 'nothing': 335,\n",
       " 'st': 336,\n",
       " 'week': 337,\n",
       " 'given': 338,\n",
       " 'bishop': 339,\n",
       " 'keep': 340,\n",
       " 'terms': 341,\n",
       " 'supreme': 342,\n",
       " 'truth': 343,\n",
       " '$': 344,\n",
       " 'israel': 345,\n",
       " 'school': 346,\n",
       " '2016': 347,\n",
       " 'congress': 348,\n",
       " 'regime': 349,\n",
       " 'didn': 350,\n",
       " 'already': 351,\n",
       " 'whether': 352,\n",
       " 'times': 353,\n",
       " 'vegas': 354,\n",
       " 'members': 355,\n",
       " 'homosexual': 356,\n",
       " 'mueller': 357,\n",
       " 'ever': 358,\n",
       " 'immigration': 359,\n",
       " 'islamic': 360,\n",
       " 'pad': 361,\n",
       " '##dock': 362,\n",
       " 'donald': 363,\n",
       " 'democrats': 364,\n",
       " 'released': 365,\n",
       " 'committee': 366,\n",
       " 'continue': 367,\n",
       " 'las': 368,\n",
       " 'whom': 369,\n",
       " 'off': 370,\n",
       " 'pass': 371,\n",
       " '##j': 372,\n",
       " '##rak': 373,\n",
       " 'agree': 374,\n",
       " 'site': 375,\n",
       " '##wee': 376,\n",
       " 'call': 377,\n",
       " '##e': 378,\n",
       " 'group': 379,\n",
       " 'di': 380,\n",
       " 'again': 381,\n",
       " '%': 382,\n",
       " 'below': 383,\n",
       " 'council': 384,\n",
       " 'name': 385,\n",
       " 'b': 386,\n",
       " 'course': 387,\n",
       " 'senate': 388,\n",
       " 'question': 389,\n",
       " 'russian': 390,\n",
       " 'twitter': 391,\n",
       " 'co': 392,\n",
       " 'am': 393,\n",
       " 'days': 394,\n",
       " 'need': 395,\n",
       " 'official': 396,\n",
       " 'done': 397,\n",
       " 'known': 398,\n",
       " '##n': 399,\n",
       " 'children': 400,\n",
       " 'authorities': 401,\n",
       " 'something': 402,\n",
       " '3': 403,\n",
       " 'put': 404,\n",
       " 'jewish': 405,\n",
       " 'change': 406,\n",
       " 'vote': 407,\n",
       " 'force': 408,\n",
       " 'control': 409,\n",
       " 'cnn': 410,\n",
       " 'claims': 411,\n",
       " 'yet': 412,\n",
       " 'cases': 413,\n",
       " 'attack': 414,\n",
       " 'barr': 415,\n",
       " 'came': 416,\n",
       " 'j': 417,\n",
       " 'video': 418,\n",
       " 'reports': 419,\n",
       " 'later': 420,\n",
       " 'victims': 421,\n",
       " '2017': 422,\n",
       " '11': 423,\n",
       " 'high': 424,\n",
       " 'north': 425,\n",
       " 'next': 426,\n",
       " 'others': 427,\n",
       " 'secretary': 428,\n",
       " 'took': 429,\n",
       " '##ted': 430,\n",
       " '##p': 431,\n",
       " '##um': 432,\n",
       " 'too': 433,\n",
       " 'give': 434,\n",
       " 've': 435,\n",
       " 'asked': 436,\n",
       " 'great': 437,\n",
       " 'mass': 438,\n",
       " 'archbishop': 439,\n",
       " '##og': 440,\n",
       " 'until': 441,\n",
       " 'n': 442,\n",
       " 'posted': 443,\n",
       " 'paul': 444,\n",
       " 'facebook': 445,\n",
       " 'berg': 446,\n",
       " 'field': 447,\n",
       " 'point': 448,\n",
       " '5': 449,\n",
       " 'post': 450,\n",
       " 'rep': 451,\n",
       " 'religious': 452,\n",
       " '9': 453,\n",
       " 'ago': 454,\n",
       " 'things': 455,\n",
       " 'corrupt': 456,\n",
       " 'priests': 457,\n",
       " 'real': 458,\n",
       " 'mr': 459,\n",
       " 'outbreak': 460,\n",
       " 'vi': 461,\n",
       " 'documents': 462,\n",
       " 'cannot': 463,\n",
       " 'doing': 464,\n",
       " 'secret': 465,\n",
       " '##es': 466,\n",
       " 'let': 467,\n",
       " '##l': 468,\n",
       " 'might': 469,\n",
       " 'defense': 470,\n",
       " 'least': 471,\n",
       " 'following': 472,\n",
       " 'consider': 473,\n",
       " 'officers': 474,\n",
       " 'director': 475,\n",
       " 'ambassador': 476,\n",
       " 'added': 477,\n",
       " 'taking': 478,\n",
       " '##h': 479,\n",
       " 'month': 480,\n",
       " 'brett': 481,\n",
       " 'among': 482,\n",
       " 'faith': 483,\n",
       " 'hillary': 484,\n",
       " 'taken': 485,\n",
       " 'around': 486,\n",
       " 'privacy': 487,\n",
       " 'iranian': 488,\n",
       " 'enforcement': 489,\n",
       " 'holy': 490,\n",
       " 'thought': 491,\n",
       " 'special': 492,\n",
       " 'number': 493,\n",
       " 'become': 494,\n",
       " 'actually': 495,\n",
       " 'health': 496,\n",
       " '##tion': 497,\n",
       " 'mexico': 498,\n",
       " '##ki': 499,\n",
       " 'allegations': 500,\n",
       " 'matter': 501,\n",
       " 'cover': 502,\n",
       " 'few': 503,\n",
       " 'true': 504,\n",
       " 'women': 505,\n",
       " '##lea': 506,\n",
       " 'sent': 507,\n",
       " 'purposes': 508,\n",
       " 'second': 509,\n",
       " '#': 510,\n",
       " 'ask': 511,\n",
       " 'due': 512,\n",
       " '##lio': 513,\n",
       " 'making': 514,\n",
       " 'possible': 515,\n",
       " 'went': 516,\n",
       " '##re': 517,\n",
       " 'help': 518,\n",
       " 'woman': 519,\n",
       " 'within': 520,\n",
       " 'show': 521,\n",
       " 'al': 522,\n",
       " 'young': 523,\n",
       " 'set': 524,\n",
       " '4': 525,\n",
       " 'family': 526,\n",
       " 'really': 527,\n",
       " 'simply': 528,\n",
       " '##ers': 529,\n",
       " 'grants': 530,\n",
       " 'anytime': 531,\n",
       " '##ist': 532,\n",
       " 'friday': 533,\n",
       " 'de': 534,\n",
       " 'nation': 535,\n",
       " 'jews': 536,\n",
       " 'updates': 537,\n",
       " 'further': 538,\n",
       " 'per': 539,\n",
       " 'legal': 540,\n",
       " 'haley': 541,\n",
       " 'away': 542,\n",
       " 'despite': 543,\n",
       " '##al': 544,\n",
       " 'powell': 545,\n",
       " 'foreign': 546,\n",
       " 'several': 547,\n",
       " 'opt': 548,\n",
       " 'page': 549,\n",
       " 'san': 550,\n",
       " 'though': 551,\n",
       " 'wednesday': 552,\n",
       " 'seen': 553,\n",
       " 'thing': 554,\n",
       " 'decision': 555,\n",
       " 'having': 556,\n",
       " 'instead': 557,\n",
       " 'coming': 558,\n",
       " 'community': 559,\n",
       " 'completing': 560,\n",
       " 'guardian': 561,\n",
       " 'guy': 562,\n",
       " 'months': 563,\n",
       " 'tuesday': 564,\n",
       " 'five': 565,\n",
       " 'intelligence': 566,\n",
       " 'unchanged': 567,\n",
       " 'plague': 568,\n",
       " 'happened': 569,\n",
       " 'monday': 570,\n",
       " 'wanted': 571,\n",
       " 'wi': 572,\n",
       " 'act': 573,\n",
       " 'validation': 574,\n",
       " 'records': 575,\n",
       " 'ice': 576,\n",
       " 'staff': 577,\n",
       " 'october': 578,\n",
       " '6': 579,\n",
       " 'syria': 580,\n",
       " 'four': 581,\n",
       " 'charges': 582,\n",
       " 'knew': 583,\n",
       " 'za': 584,\n",
       " 'working': 585,\n",
       " 'arrested': 586,\n",
       " 'claim': 587,\n",
       " 'k': 588,\n",
       " '##ks': 589,\n",
       " 'close': 590,\n",
       " 'jim': 591,\n",
       " 'questions': 592,\n",
       " 'florida': 593,\n",
       " 'city': 594,\n",
       " 'trying': 595,\n",
       " 'release': 596,\n",
       " 'wrong': 597,\n",
       " 'replace': 598,\n",
       " '##mp': 599,\n",
       " 'west': 600,\n",
       " '10': 601,\n",
       " 'themselves': 602,\n",
       " 'faithful': 603,\n",
       " 'benedict': 604,\n",
       " 'violence': 605,\n",
       " 'member': 606,\n",
       " 'process': 607,\n",
       " 'full': 608,\n",
       " 'published': 609,\n",
       " 'dr': 610,\n",
       " 'father': 611,\n",
       " 'incident': 612,\n",
       " 'important': 613,\n",
       " 'gave': 614,\n",
       " 'christ': 615,\n",
       " 'ho': 616,\n",
       " 'ms': 617,\n",
       " 'issue': 618,\n",
       " 'details': 619,\n",
       " 'hand': 620,\n",
       " 'recent': 621,\n",
       " 'look': 622,\n",
       " 'outpost': 623,\n",
       " '##en': 624,\n",
       " '##ger': 625,\n",
       " '30': 626,\n",
       " 'meeting': 627,\n",
       " 'find': 628,\n",
       " 'response': 629,\n",
       " '7': 630,\n",
       " 'anyone': 631,\n",
       " 'past': 632,\n",
       " 'assault': 633,\n",
       " 'someone': 634,\n",
       " '##stein': 635,\n",
       " 'yes': 636,\n",
       " 'muslims': 637,\n",
       " 'hai': 638,\n",
       " 'top': 639,\n",
       " 'alleged': 640,\n",
       " '##z': 641,\n",
       " 'accused': 642,\n",
       " 'situation': 643,\n",
       " 'gill': 644,\n",
       " 'sure': 645,\n",
       " 'permission': 646,\n",
       " 'robert': 647,\n",
       " 'system': 648,\n",
       " 'korea': 649,\n",
       " 'nikki': 650,\n",
       " 'p': 651,\n",
       " 'head': 652,\n",
       " '##ic': 653,\n",
       " 'fox': 654,\n",
       " '2015': 655,\n",
       " 'sin': 656,\n",
       " 'rather': 657,\n",
       " 'different': 658,\n",
       " 'doesn': 659,\n",
       " 'black': 660,\n",
       " 'crime': 661,\n",
       " 'rights': 662,\n",
       " 'along': 663,\n",
       " 'federation': 664,\n",
       " 'julian': 665,\n",
       " 'little': 666,\n",
       " 'seems': 667,\n",
       " '##os': 668,\n",
       " 'killed': 669,\n",
       " 'involved': 670,\n",
       " 'based': 671,\n",
       " 'november': 672,\n",
       " 'action': 673,\n",
       " 'best': 674,\n",
       " 'non': 675,\n",
       " '##bola': 676,\n",
       " '##us': 677,\n",
       " 'caravan': 678,\n",
       " 'speech': 679,\n",
       " 'pro': 680,\n",
       " '##v': 681,\n",
       " 'child': 682,\n",
       " 'politician': 683,\n",
       " 'face': 684,\n",
       " 'night': 685,\n",
       " 'actions': 686,\n",
       " 'authority': 687,\n",
       " 'conference': 688,\n",
       " 'million': 689,\n",
       " 'kind': 690,\n",
       " '##c': 691,\n",
       " 'republican': 692,\n",
       " '##ne': 693,\n",
       " 'means': 694,\n",
       " 'whose': 695,\n",
       " '##b': 696,\n",
       " 'book': 697,\n",
       " 'leader': 698,\n",
       " '##ha': 699,\n",
       " 'crimes': 700,\n",
       " 'pre': 701,\n",
       " 'testimony': 702,\n",
       " 'hearing': 703,\n",
       " 'big': 704,\n",
       " 'gr': 705,\n",
       " '##ism': 706,\n",
       " 'victim': 707,\n",
       " '##fi': 708,\n",
       " 'o': 709,\n",
       " 'behavior': 710,\n",
       " 'po': 711,\n",
       " 'anything': 712,\n",
       " 'continued': 713,\n",
       " 'read': 714,\n",
       " 'commission': 715,\n",
       " 'document': 716,\n",
       " 'google': 717,\n",
       " 'money': 718,\n",
       " 'mis': 719,\n",
       " 'likely': 720,\n",
       " 'got': 721,\n",
       " '##te': 722,\n",
       " 'clearly': 723,\n",
       " 'upon': 724,\n",
       " 'students': 725,\n",
       " 'gotten': 726,\n",
       " 'received': 727,\n",
       " 'reason': 728,\n",
       " 'interview': 729,\n",
       " '##f': 730,\n",
       " 'daily': 731,\n",
       " 'migrants': 732,\n",
       " 'once': 733,\n",
       " 'county': 734,\n",
       " '##le': 735,\n",
       " 'september': 736,\n",
       " '##m': 737,\n",
       " '##eo': 738,\n",
       " 'americans': 739,\n",
       " 'priest': 740,\n",
       " 'counsel': 741,\n",
       " 'problem': 742,\n",
       " 'officer': 743,\n",
       " 'neo': 744,\n",
       " 'hard': 745,\n",
       " 'murder': 746,\n",
       " 'fire': 747,\n",
       " 'sanctions': 748,\n",
       " 'assassination': 749,\n",
       " '##pl': 750,\n",
       " 'human': 751,\n",
       " 'laws': 752,\n",
       " 'words': 753,\n",
       " 'thursday': 754,\n",
       " '##man': 755,\n",
       " 'la': 756,\n",
       " 'york': 757,\n",
       " '##ful': 758,\n",
       " 'comes': 759,\n",
       " 'efforts': 760,\n",
       " 'amendment': 761,\n",
       " 'room': 762,\n",
       " '##gano': 763,\n",
       " 'began': 764,\n",
       " 'serious': 765,\n",
       " 'whole': 766,\n",
       " 'position': 767,\n",
       " 'shot': 768,\n",
       " 'evil': 769,\n",
       " 'event': 770,\n",
       " 'tell': 771,\n",
       " 'constitution': 772,\n",
       " 'source': 773,\n",
       " '##li': 774,\n",
       " 'mana': 775,\n",
       " '##fort': 776,\n",
       " 'society': 777,\n",
       " 'global': 778,\n",
       " 'allowed': 779,\n",
       " 'immediately': 780,\n",
       " 'seminar': 781,\n",
       " 'social': 782,\n",
       " 'indeed': 783,\n",
       " 'enough': 784,\n",
       " 'always': 785,\n",
       " 'organization': 786,\n",
       " 'guns': 787,\n",
       " 'isn': 788,\n",
       " 'attacks': 789,\n",
       " 'forward': 790,\n",
       " 'europe': 791,\n",
       " 'senator': 792,\n",
       " 'china': 793,\n",
       " 'aw': 794,\n",
       " 'itself': 795,\n",
       " 'mu': 796,\n",
       " 'perhaps': 797,\n",
       " 'countries': 798,\n",
       " 'teaching': 799,\n",
       " 'claimed': 800,\n",
       " 'af': 801,\n",
       " 'everything': 802,\n",
       " '15': 803,\n",
       " 'earlier': 804,\n",
       " 'disease': 805,\n",
       " 'issued': 806,\n",
       " 'en': 807,\n",
       " 'calling': 808,\n",
       " 'six': 809,\n",
       " 'age': 810,\n",
       " 'saudi': 811,\n",
       " 'orb': 812,\n",
       " 'groups': 813,\n",
       " '##able': 814,\n",
       " 'toward': 815,\n",
       " 'search': 816,\n",
       " 'sex': 817,\n",
       " 'able': 818,\n",
       " 'false': 819,\n",
       " 'foundation': 820,\n",
       " 'fired': 821,\n",
       " 'fein': 822,\n",
       " 'either': 823,\n",
       " 'weapons': 824,\n",
       " 'dec': 825,\n",
       " '##ran': 826,\n",
       " '##ble': 827,\n",
       " 'lives': 828,\n",
       " 'outside': 829,\n",
       " 'chief': 830,\n",
       " 'previously': 831,\n",
       " '##ness': 832,\n",
       " 'personal': 833,\n",
       " 'provide': 834,\n",
       " 'less': 835,\n",
       " '##ity': 836,\n",
       " 'comments': 837,\n",
       " 'warrant': 838,\n",
       " 'move': 839,\n",
       " 'compound': 840,\n",
       " 'smith': 841,\n",
       " 'held': 842,\n",
       " 'team': 843,\n",
       " 'major': 844,\n",
       " 'certainly': 845,\n",
       " 'university': 846,\n",
       " 'aliens': 847,\n",
       " 'example': 848,\n",
       " 'jean': 849,\n",
       " 'committed': 850,\n",
       " 'scandal': 851,\n",
       " 'present': 852,\n",
       " 'nun': 853,\n",
       " '##ment': 854,\n",
       " 'sha': 855,\n",
       " 'operation': 856,\n",
       " 'peter': 857,\n",
       " '##ion': 858,\n",
       " 'll': 859,\n",
       " 'mind': 860,\n",
       " 'air': 861,\n",
       " 'threat': 862,\n",
       " '12': 863,\n",
       " 'inc': 864,\n",
       " 'embassy': 865,\n",
       " 'district': 866,\n",
       " '16': 867,\n",
       " 'christian': 868,\n",
       " 'weeks': 869,\n",
       " 'protect': 870,\n",
       " '50': 871,\n",
       " 'led': 872,\n",
       " 'g': 873,\n",
       " 'fight': 874,\n",
       " 'recently': 875,\n",
       " 'tried': 876,\n",
       " 'im': 877,\n",
       " 'crisis': 878,\n",
       " 'idea': 879,\n",
       " 'lot': 880,\n",
       " 'apparently': 881,\n",
       " 'soon': 882,\n",
       " 'ap': 883,\n",
       " 'decades': 884,\n",
       " 'sign': 885,\n",
       " 'light': 886,\n",
       " 'july': 887,\n",
       " '##ti': 888,\n",
       " 'attempt': 889,\n",
       " '20': 890,\n",
       " 'each': 891,\n",
       " 'ab': 892,\n",
       " 'hate': 893,\n",
       " 'investigators': 894,\n",
       " 'sub': 895,\n",
       " '8': 896,\n",
       " 'heard': 897,\n",
       " 'ammunition': 898,\n",
       " 'met': 899,\n",
       " 'immigrants': 900,\n",
       " '##k': 901,\n",
       " 'middle': 902,\n",
       " 'schools': 903,\n",
       " '##ut': 904,\n",
       " 'ban': 905,\n",
       " 'front': 906,\n",
       " 'regarding': 907,\n",
       " 'entire': 908,\n",
       " 'agency': 909,\n",
       " 'pic': 910,\n",
       " 'latest': 911,\n",
       " '13': 912,\n",
       " 'spread': 913,\n",
       " 'keith': 914,\n",
       " 'rec': 915,\n",
       " 'finally': 916,\n",
       " 'local': 917,\n",
       " 'rome': 918,\n",
       " 'prison': 919,\n",
       " 'living': 920,\n",
       " 'bring': 921,\n",
       " 'prior': 922,\n",
       " 'longer': 923,\n",
       " 'role': 924,\n",
       " 'often': 925,\n",
       " 'muhammad': 926,\n",
       " 'wasn': 927,\n",
       " 'hope': 928,\n",
       " 'care': 929,\n",
       " 'ii': 930,\n",
       " 'guilty': 931,\n",
       " 'uranium': 932,\n",
       " 'future': 933,\n",
       " 'bill': 934,\n",
       " 'key': 935,\n",
       " '##ies': 936,\n",
       " 'described': 937,\n",
       " '##ry': 938,\n",
       " 'word': 939,\n",
       " 'using': 940,\n",
       " 'failed': 941,\n",
       " 'imp': 942,\n",
       " 'nor': 943,\n",
       " 'fa': 944,\n",
       " 'december': 945,\n",
       " 'voters': 946,\n",
       " 'synod': 947,\n",
       " 'religion': 948,\n",
       " 'thus': 949,\n",
       " 'col': 950,\n",
       " 'bush': 951,\n",
       " 'sa': 952,\n",
       " 'cause': 953,\n",
       " '&': 954,\n",
       " 'cr': 955,\n",
       " 'catholics': 956,\n",
       " 'lawyer': 957,\n",
       " 'refused': 958,\n",
       " 'march': 959,\n",
       " 'lawsuit': 960,\n",
       " 'result': 961,\n",
       " 'born': 962,\n",
       " 'plan': 963,\n",
       " 'allow': 964,\n",
       " 'red': 965,\n",
       " 'remain': 966,\n",
       " '2013': 967,\n",
       " 'remember': 968,\n",
       " 'mainstream': 969,\n",
       " 'phone': 970,\n",
       " 'almost': 971,\n",
       " '##ni': 972,\n",
       " 'attention': 973,\n",
       " 'stand': 974,\n",
       " 'uk': 975,\n",
       " 'job': 976,\n",
       " 'above': 977,\n",
       " 'publicly': 978,\n",
       " 'note': 979,\n",
       " 'open': 980,\n",
       " 'makes': 981,\n",
       " 'lord': 982,\n",
       " 'monastery': 983,\n",
       " 'conservative': 984,\n",
       " 'asking': 985,\n",
       " 'talk': 986,\n",
       " 'won': 987,\n",
       " 'conspiracy': 988,\n",
       " '##ler': 989,\n",
       " 'em': 990,\n",
       " 'current': 991,\n",
       " 'especially': 992,\n",
       " 'ordered': 993,\n",
       " 'isis': 994,\n",
       " 'behind': 995,\n",
       " 'w': 996,\n",
       " 'apartment': 997,\n",
       " 'certain': 998,\n",
       " 'follow': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('datasets/vocab.pickle', 'rb') as handle:\n",
    "    vocabulary = pickle.load(handle)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2725f2030e4a53b9c4abf4a80db899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13485.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-62d6138b9b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbembeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcembeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^A-Za-z0-9 ]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5118ecda0e7b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpadded_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "token_df = []\n",
    "for sentence, tokens, labels in tzip(X, X_new, Y_new):\n",
    "    assert len(tokens)==len(labels)\n",
    "    bembeddings = torch.tensor(bert.transform([sentence])[0])[:,768:][:,768*11:768*12]\n",
    "    cembeddings = custom.transform([sentence])[0][:,768:][:,768*11:768*12]\n",
    "    words = re.sub(r'[^A-Za-z0-9 ]+', '', sentence).split(\" \")\n",
    "    assert bembeddings.shape==cembeddings.shape\n",
    "    for i,token in enumerate(tokens):\n",
    "        cosine_value, euclid_value = measure_similarity(bembeddings[i,:], cembeddings[i,:])\n",
    "        if token.isalpha():\n",
    "            is_word = 1\n",
    "        else:\n",
    "            is_word = 0\n",
    "        token_df.append([token, is_word, cosine_value, euclid_value, labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df = pd.DataFrame.from_records(token_df)\n",
    "df.columns = [\"token\", \"is_word\",\"cosine_value\",\"euclid_value\",\"label\"]\n",
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std(df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df.loc[df[\"token\"].isin(list(vocabulary.keys())[100:])]\n",
    "filtered.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std(filtered, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
